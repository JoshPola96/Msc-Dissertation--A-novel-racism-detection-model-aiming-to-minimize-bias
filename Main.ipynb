{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCL9h2rkLUbS"
      },
      "source": [
        "This is the code implementation of the work for my MSc dissertation preparing a Racism detection model for tweets exploring avenues to reduce bias and demographic contexts. Please Note that only functional code snippets are provided rather than actual working implementation and duplication of steps are not presented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwV3lgXvCqdq"
      },
      "source": [
        "#used in pre-processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import sys\n",
        "!pip install dataprep\n",
        "from dataprep.clean import clean_country\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION,p.OPT.HASHTAG,p.OPT.RESERVED,p.OPT.NUMBER)\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "!pip install ekphrasis\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "seg = Segmenter(corpus=\"twitter\") \n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        " \n",
        "#used in EDA\n",
        "!pip install chart_studio\n",
        "!pip install textstat\n",
        "from sklearn import model_selection\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#used in model building\n",
        "!pip install tensorflow_text\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from tensorflow.python.keras import regularizers\n",
        "!pip install -q tf-models-official\n",
        "from official.nlp import optimization\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.layers import Activation\n",
        "from keras.layers import concatenate\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.utils import class_weight\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sys.setrecursionlimit(22000)\n",
        "\n",
        "'''\n",
        "pd.set_option('display.max_rows', 300)\n",
        "pd.set_option('display.max_columns', 300)\n",
        "pd.set_option('display.width', 300)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uueulFs8Fmcf"
      },
      "source": [
        "Altering File 1 from Kaggle: https://kaggle.com/raghadabdullah/racism-tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB30LFaNEWg1"
      },
      "source": [
        "#upload files\n",
        "from google.colab import files \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Woioz1-EZEz"
      },
      "source": [
        "#reading to a df\n",
        "tweets=pd.read_csv('tweets_PB1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW1aztfEsDdh"
      },
      "source": [
        "#mounting google drive with colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkXaHPPxsOl6"
      },
      "source": [
        "#loading trained-model from the drive\n",
        "model_text = tf.keras.models.load_model('/content/drive/MyDrive/bert_cnn_bilst_low')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flsK3J-qEcLM"
      },
      "source": [
        "#dropping and renaming columns\n",
        "formatted1_df = unformatted_df.drop(columns=['index','oh_label','id'])\n",
        "formatted1_df=formatted1_df.rename(columns={'Text':'text','Annotation':'annotation'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrbzjv3oFsi5"
      },
      "source": [
        "Altering File 2 from Kaggle: https://kaggle.com/munkialbright/classified-tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqqeOYS1FvgL"
      },
      "source": [
        "#dropping and renaming columns\n",
        "formatted2_df = unformatted_df1.drop(columns=['suspicious','hate','suicidal'])\n",
        "formatted2_df=formatted2_df.rename(columns={'cyberbullying':'annotation'})\n",
        "\n",
        "#selecting racism/none categories\n",
        "formatted2_df=formatted2_df.loc[formatted2_df['annotation'].isin(['0','1'])]\n",
        "\n",
        "#relabelling 0:none, 1:racism\n",
        "formatted2_df[\"annotation\"]=formatted2_df[\"annotation\"].replace({0:'none',1:'racism'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dg6O_5jF_qo"
      },
      "source": [
        "#merging both dfs and droppping index\n",
        "final = pd.merge(formatted1_df, formatted2_df, on='text')\n",
        "final=final.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySSZccZRIql-"
      },
      "source": [
        "#removing duplicate texts\n",
        "formatted2_df = formatted2_df.drop_duplicates('text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nw3BiTdGNho"
      },
      "source": [
        "#saving as csv\n",
        "final.to_csv('racism_P.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU3uxmloIzQV"
      },
      "source": [
        "#read tsv files\n",
        "tweets=pd.read_csv('waseem_tweets.csv',delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KehTO2waJnyQ"
      },
      "source": [
        "Altering annotations and downloaded tweets: https://github.com/ZeerakW/hatespeech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIxZOmfoI7Im"
      },
      "source": [
        "#creating a label from four annotators\n",
        "labels['annotation'] = labels['Expert'].str.contains('racism') | labels['Amateur_0'].str.contains('racism') & labels['Amateur_1'].str.contains('racism') & labels['Amateur_2'].str.contains('racism')\n",
        "labels['annotation'] = labels['annotation'].map({True: 'racism', False: 'none'})\n",
        "\n",
        "#stripping spaces\n",
        "tweets.columns = tweets.columns.str.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VoIziNCJ91D"
      },
      "source": [
        "Altering tweet file downloaded using Hydrator (Twitter API)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqwC4DYHJ2vJ"
      },
      "source": [
        "#dropping unneded columns\n",
        "tweets = tweets.drop(columns=['coordinates','created_at','hashtags','media','urls','favorite_count','in_reply_to_screen_name','in_reply_to_status_id','in_reply_to_user_id','lang','place','possibly_sensitive','retweet_count','retweet_id','retweet_screen_name','source','tweet_url','user_created_at','user_default_profile_image','user_favourites_count','user_listed_count','user_screen_name.1','user_statuses_count','user_time_zone','user_urls','user_verified'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKjDAObPKj7X"
      },
      "source": [
        "#merging tweet and labels\n",
        "waseemhovey_final=pd.merge(tweets, labels, on='id', how='outer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmagHNk2KuBx"
      },
      "source": [
        "#appending both merged waseem & hovy datasets\n",
        "wh_final = waseem_final.append(waseemhovey_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BW_LOfiKocX"
      },
      "source": [
        "#dropping tweets with null texts\n",
        "tweets=wh_final.dropna(subset = [\"text\"])\n",
        "#removing duplicate texts\n",
        "tweets = tweets.drop_duplicates('text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m43BYX9UMo4i"
      },
      "source": [
        "Pre-processing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKGnalpvMjJy"
      },
      "source": [
        "#filtering non-english and extremely undecipherable tweets\n",
        "\n",
        "#creates a new column with language name\n",
        "def detect_lang(txt):\n",
        "  try:\n",
        "    return detect(txt)\n",
        "  except:\n",
        "    return np.nan\n",
        "tweetstmp[\"language\"] = tweetstmp.text.apply(detect_lang)\n",
        "\n",
        "#selecting english tweets\n",
        "tweetstmp = tweetstmp[tweetstmp.language == \"en\"]\n",
        "tweetstmp=tweetstmp.reset_index(drop = True)\n",
        "\n",
        "#dropping extra columns\n",
        "tweetstmp=tweetstmp.drop(columns=['language'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OdpD_U2NKSK"
      },
      "source": [
        "#tweet processor to apply tweet cleaning\n",
        "def preprocess_tweet(row):\n",
        "    text = row['text']\n",
        "    text = p.clean(text)\n",
        "    return text\n",
        "tweetstmp['text'] = tweetstmp.apply(preprocess_tweet, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN150JfKNkVs"
      },
      "source": [
        "#removing special characters, numbers, whitespaces etc\n",
        "tweetstmp['text'] = tweetstmp['text'].str.replace('[^\\w\\s]','').str.replace('\\s\\s+', ' ').str.replace('\\[!.*?\\]', '').str.replace('\\n', '').str.replace('\\w*\\d\\w*', '').str.replace('\\d+', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc6ptqgINYmy"
      },
      "source": [
        "#word segmentation\n",
        "tweetstmp['text']=[seg.segment(tweetstmp.loc[i,'text']) for i in range(len(tweetstmp))]\n",
        "tweetstmp['user_description']=[seg.segment(tweetstmp.loc[i,'user_description']) for i in range(len(tweetstmp))]\n",
        "tweetstmp['user_name']=[seg.segment(tweetstmp.loc[i,'user_name']) for i in range(len(tweetstmp))]\n",
        "tweetstmp['user_screen_name']=[seg.segment(tweetstmp.loc[i,'user_screen_name']) for i in range(len(tweetstmp))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmiQlnR-Nx0Q"
      },
      "source": [
        "#extracting standardised locations\n",
        "\n",
        "#returns new column with location\n",
        "tweetstmp=clean_country(tweetstmp, 'user_location')\n",
        "\n",
        "#dropping old field and renaming new one\n",
        "tweetstmp = tweetstmp.drop(columns=['user_location'])\n",
        "tweetstmp=tweetstmp.rename(columns={'user_location_clean':'user_location'})\n",
        "\n",
        "#lowercasing\n",
        "tweetstmp['user_location']=tweetstmp['user_location'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yupFSw0POODR"
      },
      "source": [
        "#sentiment analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "#new column with compound scores\n",
        "tweetstmp['compound'] = [analyzer.polarity_scores(x)['compound'] for x in tweetstmp['text']]\n",
        "\n",
        "#labelling score values to sentiment labels\n",
        "conditions = [\n",
        "    (tweetstmp['compound'] >= 0.05),\n",
        "    (tweetstmp['compound'] <= - 0.05)\n",
        "    ]\n",
        "values = ['Positive', 'Negative']\n",
        "tweetstmp['sentiment'] = np.select(conditions, values)\n",
        "\n",
        "tweetstmp['sentiment']=tweetstmp['sentiment'].replace('0','neutral')\n",
        "\n",
        "#lowercasing & dropping extra column\n",
        "tweetstmp['sentiment']=tweetstmp['sentiment'].str.lower()\n",
        "tweetstmp = tweetstmp.drop(columns=['compound'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll6umyFOMiTs"
      },
      "source": [
        "#dropping non-essential text features for reducing skew of the model due to low size and imbalanced dataset\n",
        "tweetstmp = tweetstmp.drop(columns=['user_screen_name','user_description','user_name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH6YQVy2OprD"
      },
      "source": [
        "#binarizing labels\n",
        "tweetstmp['annotation']=np.where(tweetstmp['annotation']=='racism', 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH435F69O5jJ"
      },
      "source": [
        "#handling nans\n",
        "tweetstmp.fillna(value='', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suxhPtPR5qct"
      },
      "source": [
        "#one hot enocding\n",
        "one_hot = pd.get_dummies(tweets['sentiment'])\n",
        "tweets = tweets.drop('sentiment',axis = 1)\n",
        "tweets = tweets.join(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN98M14K5ytu"
      },
      "source": [
        "#normalization using quantile transformer\n",
        "qt = QuantileTransformer()\n",
        "tweets_nn = pd.DataFrame(qt.fit_transform(tweets_nn),columns = tweets_nn.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6fKbFn2PHCM"
      },
      "source": [
        "EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueH0TLlVPmvu"
      },
      "source": [
        "#displaying a sample of each label\n",
        "print(\"Racist Tweet example :\",tweets[tweets['annotation']==1]['text'].values[8])\n",
        "print(\"Non-Racist Tweet example :\",tweets[tweets['annotation']==0]['text'].values[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYoW2nPdPtY7"
      },
      "source": [
        "#count of the features\n",
        "tweets.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqRTUNPyPyCA"
      },
      "source": [
        "#distribution of the dataset\n",
        "tweets['annotation'].value_counts()\n",
        "tweets['annotation'].value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ara533gP7ZU"
      },
      "source": [
        "#distribution of sentiment features for each label\n",
        "tweets.groupby([\"annotation\", \"sentiment\"]).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK53OYuKQHF7"
      },
      "source": [
        "#distribution of locations per label\n",
        "tweets.groupby([\"annotation\", \"user_location\"]).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ku8Zl1wQNYs"
      },
      "source": [
        "#creating separate dfs for each labels\n",
        "racist = tweets[tweets['annotation']==1]\n",
        "none = tweets[tweets['annotation']==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FY7Y5CDQUNL"
      },
      "source": [
        "#function to return top words\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    \"\"\"\n",
        "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
        "    \"\"\"\n",
        "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "#function to return top n-grams\n",
        "def get_top_n_gram(corpus,ngram_range,n=None):\n",
        "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XanG8CiQhq6"
      },
      "source": [
        "#top 10 words in racist tweets\n",
        "rac_unigrams = get_top_n_words(racist['text'],10)\n",
        "\n",
        "#top 10 words in normal tweets\n",
        "non_unigrams = get_top_n_words(none['text'],10)\n",
        "\n",
        "#top 10 racist bigrams\n",
        "rac_bigrams = get_top_n_gram(racist['text'],(2,2),10)\n",
        "\n",
        "#top 10 normal bigrams\n",
        "non_bigrams = get_top_n_gram(none['text'],(2,2),10)\n",
        "\n",
        "#top 10 racist trigrams\n",
        "rac_trigrams = get_top_n_gram(racist['text'],(3,3),10)\n",
        "\n",
        "#top 10 normal trigrams\n",
        "non_trigrams = get_top_n_gram(none['text'],(3,3),10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xXzzwLPQ0VO"
      },
      "source": [
        "#word cloud\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 10])\n",
        "wordcloud1 = WordCloud( background_color='white',\n",
        "                        width=300,\n",
        "                        height=250).generate(\" \".join(none['text']))\n",
        "ax1.imshow(wordcloud1)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Normal Tweets',fontsize=30);\n",
        "\n",
        "wordcloud2 = WordCloud( background_color='white',\n",
        "                        width=300,\n",
        "                        height=250).generate(\" \".join(racist['text']))\n",
        "ax2.imshow(wordcloud2)\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Racist Tweets',fontsize=30);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lafZyQxCSvzc"
      },
      "source": [
        "Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASVHLvlQSyfW"
      },
      "source": [
        "#downloading and loading Bert pre-processor and encoder from Tensorflow\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E9302q5S9ra"
      },
      "source": [
        "#function to test the encoder. returns word embeddings as tensors\n",
        "def get_sentence_embeding(sentences):\n",
        "    preprocessed_text = bert_preprocess(sentences)\n",
        "    return bert_encoder(preprocessed_text)['pooled_output']    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20AlqNdRTUx-"
      },
      "source": [
        "#splitting data for test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(tweets['text'],tweets['annotation'],stratify=tweets['annotation'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqM-EnQG3I6V"
      },
      "source": [
        "Text Model-BERT+CNN+bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVzuLffH3L89"
      },
      "source": [
        "#functional model\n",
        "\n",
        "#bert word embeddings\n",
        "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text_inputs')\n",
        "preprocessed_text = bert_preprocess(text_input)\n",
        "text_outputs = bert_encoder(preprocessed_text)\n",
        "out=(text_outputs['pooled_output'])\n",
        "\n",
        "#reshaping 2D tensor to 3D for the model\n",
        "out1=tf.keras.layers.Reshape([1024,1], name='reshape1')(out)\n",
        "\n",
        "#cnn layers\n",
        "cnn1 = keras.layers.Conv1D(filters=64,kernel_size=2, activation='relu', padding='same', kernel_regularizer=keras.regularizers.l2(0.1), name='cnn1')(out1)\n",
        "cnn2 = keras.layers.Conv1D(filters=64,kernel_size=3, activation='relu', padding='same', kernel_regularizer=keras.regularizers.l2(0.1), name='cnn2')(out1)\n",
        "cnn3 = keras.layers.Conv1D(filters=64,kernel_size=4, activation='relu', padding='same', kernel_regularizer=keras.regularizers.l2(0.1), name='cnn3')(out1)\n",
        "cnn4 = keras.layers.Conv1D(filters=64,kernel_size=5, activation='relu', padding='same', kernel_regularizer=keras.regularizers.l2(0.1), name='cnn4')(out1)\n",
        "\n",
        "#maxpooling layers\n",
        "pool1=tf.keras.layers.MaxPooling1D(pool_size=2,name='pool1')(cnn1)\n",
        "pool2=tf.keras.layers.MaxPooling1D(pool_size=2,name='pool2')(cnn2)\n",
        "pool3=tf.keras.layers.MaxPooling1D(pool_size=2,name='pool3')(cnn3)\n",
        "pool4=tf.keras.layers.MaxPooling1D(pool_size=2,name='pool4')(cnn4)\n",
        "\n",
        "#dropout layers\n",
        "d1 = tf.keras.layers.Dropout(rate=0.1, name=\"dropout1\")(pool1)\n",
        "d2 = tf.keras.layers.Dropout(rate=0.1, name=\"dropout2\")(pool2)\n",
        "d3 = tf.keras.layers.Dropout(rate=0.1, name=\"dropout3\")(pool3)\n",
        "d4 = tf.keras.layers.Dropout(rate=0.1, name=\"dropout4\")(pool4)\n",
        "\n",
        "#concatenating cnn outputs\n",
        "concat=tf.keras.layers.concatenate([d1,d2,d3,d4], name='concat')\n",
        "\n",
        "#bi-lstm layer\n",
        "bi1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32),  name='bi-lstm1')(concat)\n",
        "\n",
        "#dropout layer\n",
        "d5 = tf.keras.layers.Dropout(rate=0.1, name=\"dropout5\")(bi1)\n",
        "\n",
        "#classification dense layer\n",
        "dns1 = tf.keras.layers.Dense(units=1, activation='sigmoid', name=\"bert_outputs\")(d5)\n",
        "\n",
        "#model definition\n",
        "model_text = tf.keras.Model(inputs=[text_input], outputs = [dns1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmDglBz936_N"
      },
      "source": [
        "#summary of the model\n",
        "model_text.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYewsQiY4BZM"
      },
      "source": [
        "#defining optimizer and learning rate\n",
        "opt = tf.keras.optimizers.Adam(5e-5)\n",
        "\n",
        "#defining an early stoping function to control epochs based on model loss\n",
        "es = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "#compiling model\n",
        "model_text.compile(optimizer=opt, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "#defining class weights to counter label distibution imbalance\n",
        "class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train), y_train))) \n",
        "\n",
        "#training the model\n",
        "bert_cnn_bilstm=model_text.fit(x_train,y_train, epochs=100, batch_size=32, class_weight=class_weights, verbose=1, callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL18uo644ft8"
      },
      "source": [
        "#reset learned weights\n",
        "model_text.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eda8eDjP4lHl"
      },
      "source": [
        "#saving trained model\n",
        "model_text.tf.keras.models.save('bert_cnn_bilstm_low')\n",
        "\n",
        "#loading saved model\n",
        "model_text = tf.keras.models.load_model('bert_cnn_bilstm_low')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be8lX0x6443k"
      },
      "source": [
        "#evaluating model on test set\n",
        "model_text.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s77HUC4I487E"
      },
      "source": [
        "#sample real-world predictions\n",
        "sample_text = (['the bengalis are hoarding the state and littering it',\n",
        "                'hello darling',\n",
        "                'deport your black ass back to Africa nigga',\n",
        "                'tommorrow is a good day',\n",
        "                'live and let live says a wise man',\n",
        "                'people deserve to live',\n",
        "                'asians are shit and should be kept in their offices',\n",
        "                'Messis debut is something to look forward to',\n",
        "                'The guys wont be back until these German twits leave the table'])\n",
        "predictions = model_text.predict(sample_text)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oY-GAtf5Ivs"
      },
      "source": [
        "#function to return roc_auc score\n",
        "def roc_auc(predictions,target):\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    return roc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVjFvgEX5NCs"
      },
      "source": [
        "#running predictions and binarizing result\n",
        "predictions = model_text.predict(x_test)\n",
        "predictions=np.where(predictions>0.5,1,0)\n",
        "\n",
        "#confusion matrix\n",
        "results = confusion_matrix(y_test, predictions)\n",
        "print(results)\n",
        "\n",
        "#classification report\n",
        "print(classification_report(y_test,predictions))\n",
        "\n",
        "#roc_auc score\n",
        "scores_model = []\n",
        "scores_model.append({'Model': 'Bert_CNN_biLSTM','AUC_Score': round(roc_auc(model_text.predict(x_test),y_test),2)})\n",
        "scores_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSayw1c75_j_"
      },
      "source": [
        "Numerical Model-Dense Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moJACHUM6Fct"
      },
      "source": [
        "#sequential model\n",
        "num_model = Sequential()\n",
        "num_model.add(tf.keras.layers.Dense(128, activation='relu',name ='dense1'))\n",
        "num_model.add(tf.keras.layers.Dropout(rate=0.1, name='drop1'))\n",
        "num_model.add(tf.keras.layers.Dense(1, activation='sigmoid', name='dense2'))\n",
        "num_model.compile(loss='binary_crossentropy',optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tyIS9dx6X0d"
      },
      "source": [
        "Combined Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V--HjK866dA9"
      },
      "source": [
        "#splitting data for combined model(3 inputs)\n",
        "x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(tweets['text'],tweets[tweets.columns.difference(['text', 'annotation'])], tweets['annotation'],stratify=tweets['annotation'], test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t1ccC3P6um2"
      },
      "source": [
        "#combining trained model outputs\n",
        "combined = concatenate([model_text.output, num_model.output])\n",
        "\n",
        "#layers\n",
        "fdense1 = tf.keras.layers.Dense(256, activation=\"relu\", name ='fdense1')(combined)\n",
        "fd1= tf.keras.layers.Dropout(rate=0.1, name='fdrop1')(fdense1)\n",
        "fdense2 = tf.keras.layers.Dense(1, activation=\"sigmoid\", name ='fdense2')(fd1)\n",
        "\n",
        "#defining model with inputs of pre-built model combined with the final layers\n",
        "combined_model = tf.keras.Model(inputs=[model_text.input, num_model.input], outputs=fdense2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSxQqYE67HMu"
      },
      "source": [
        "#training combined model\n",
        "final_nn=combined_model.fit([x_train, y_train], z_train, epochs=100, batch_size=32, class_weight=class_weights, verbose=1, callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB2TOAPL7PK9"
      },
      "source": [
        "#evaluating combined model\n",
        "combined_model.evaluate([x_test,y_test],z_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqx07xUs7R6s"
      },
      "source": [
        "#running predictions and binarizing result\n",
        "predictions = model_text.predict([x_test,y_test])\n",
        "predictions=np.where(predictions>0.5,1,0)\n",
        "\n",
        "#confusion matrix\n",
        "results = confusion_matrix(z_test, predictions)\n",
        "print(results)\n",
        "\n",
        "#classification report\n",
        "print(classification_report(z_test, predictions))\n",
        "\n",
        "#roc_auc score\n",
        "scores_model = []\n",
        "scores_model.append({'Model': 'Dense','AUC_Score': round(roc_auc(combined_model.predict([x_test,y_test]),z_test),2)})\n",
        "scores_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}